# ğŸš€ DÃ©tection d'Anomalies dans la Consommation d'Ã‰nergie

### Langage

![Python Badge](https://img.shields.io/badge/python-3.9-blue)

### Frameworks et Outils de DÃ©veloppement

![Streamlit Badge](https://img.shields.io/badge/Streamlit-1.38.0-orange)
![MLFlow Badge](https://img.shields.io/badge/MLFlow-2.16.0-orange)
![Flask Badge](https://img.shields.io/badge/Flask-3.0.3-black)
![Docker Badge](https://img.shields.io/badge/Docker-Enabled-blue)
![Kedro Badge](https://img.shields.io/badge/Kedro-0.19.8-brightgreen)
![Pre-commit Badge](https://img.shields.io/badge/Pre--Commit--Hooks-4.6.0-blue)
![GitPython Badge](https://img.shields.io/badge/GitPython-3.1.43-orange)

### Machine Learning & Data Science

![MLFlow Badge](https://img.shields.io/badge/MLFlow-2.16.0-orange)
![Pandas Badge](https://img.shields.io/badge/Pandas-2.2.2-brightgreen)
![Scikit-Learn Badge](https://img.shields.io/badge/Scikit--learn-1.5.1-orange)
![Matplotlib Badge](https://img.shields.io/badge/Matplotlib-3.9.2-blue)
![Plotly Badge](https://img.shields.io/badge/Plotly-5.24.0-blue)
![KMeans Badge](https://img.shields.io/badge/KMeans-Clustering-yellow)
![DBSCAN Badge](https://img.shields.io/badge/DBSCAN-Clustering-brightgreen)
![Isolation Forest Badge](https://img.shields.io/badge/Isolation--Forest-Anomaly%20Detection-blue)
![Ridge Regression Badge](https://img.shields.io/badge/Ridge--Regression-Prediction-lightblue)
![Random Forest Badge](https://img.shields.io/badge/Random--Forest-Prediction-brightgreen)

### Cloud & Stockage

![Google Cloud Storage Badge](https://img.shields.io/badge/Google%20Cloud%20Storage-2.18.2-orange)
![Elasticsearch Badge](https://img.shields.io/badge/Elasticsearch-8.15.0-grey)
![PyMongo Badge](https://img.shields.io/badge/PyMongo-4.8.0-green)
![Boto3 Badge](https://img.shields.io/badge/Boto3-1.35.14-green)

### CI/CD et Outils de DÃ©bogage

![Pre-Commit Badge](https://img.shields.io/badge/Pre--Commit--Hooks-4.6.0-blue)
![Pylint Badge](https://img.shields.io/badge/Pylint-Enabled-green)
![Isort Badge](https://img.shields.io/badge/Isort-Enabled-brightgreen)
![Black Badge](https://img.shields.io/badge/Black-Enabled-black)
![GitHub Actions Badge](https://img.shields.io/badge/GitHub--Actions-CI%2FCD-brightgreen)


## ğŸ“‘ Sommaire
1. [ğŸ” Contexte du Projet](#1-contexte-du-projet)
2. [ğŸ¯ Pourquoi ce projet ?](#2-pourquoi-ce-projet)
3. [ğŸ¯ Objectifs du Projet](#3-objectifs-du-projet)
4. [ğŸ—ï¸ Architecture du Projet](#4-architecture-du-projet)
5. [âš™ï¸ IntÃ©gration Continue (CI) et Tests Unitaires](#5-intÃ©gration-continue-ci-et-tests-unitaires)
6. [ğŸ“‚ Structure du Projet](#6-structure-du-projet)
7. [ğŸ”„ Pipelines de Collecte de DonnÃ©es avec Kedro](#7-pipelines-de-collecte-de-donnÃ©es-avec-kedro)
8. [ğŸ’» Traitement des DonnÃ©es et Utilisation de Google Colab](#8-traitement-des-donnÃ©es-et-utilisation-de-google-colab)
9. [ğŸ¤– ModÃ¨les de Machine Learning](#9-modÃ¨les-de-machine-learning)
10. [ğŸ–¥ï¸ Interface Utilisateur avec Streamlit](#10-interface-utilisateur-avec-streamlit)
11. [ğŸ“Š Ordonnancement des DonnÃ©es avec Airflow](#11-ordonnancement-des-donnÃ©es-avec-airflow)

---

## 1. ğŸ” Contexte du Projet
La dÃ©tection prÃ©coce des anomalies Ã©nergÃ©tiques est essentielle pour la gestion proactive de l'Ã©nergie, en particulier pendant les pÃ©riodes de forte demande (hiver, Ã©tÃ©) ou durant des Ã©vÃ©nements comme les mouvements sociaux. Ce projet vise Ã  identifier ces anomalies en se basant sur des donnÃ©es variÃ©es (mÃ©tÃ©orologiques, sociales, etc.) et Ã  fournir une interface utilisateur permettant la visualisation et l'analyse des rÃ©sultats. La solution est structurÃ©e en plusieurs sous-projets interconnectÃ©s, chacun avec des objectifs spÃ©cifiques.

## 2. ğŸ¯ Pourquoi ce projet ?
Nous avons identifiÃ© plusieurs dÃ©fis majeurs dans la gestion Ã©nergÃ©tique :
- **Surconsommation en Hiver et en Ã‰tÃ©** : Augmentation significative de la demande en Ã©lectricitÃ© pendant les pÃ©riodes de froid ou de chaleur extrÃªme.
- **Jours FÃ©riÃ©s et Mouvements Sociaux** : Les variations imprÃ©vues dans la consommation peuvent dÃ©sÃ©quilibrer l'offre et la demande.

Nos cibles principales incluent :
- **Entreprises d'Ã‰lectricitÃ©** (comme Engie et EDF) qui doivent surveiller la consommation en temps rÃ©el et ajuster leur production.
- **Industries Ã  Forte Consommation** qui nÃ©cessitent une gestion optimale de leur Ã©nergie pour Ã©viter des interruptions.

## 3. ğŸ¯ Objectifs du Projet
Notre projet se concentre sur six grands objectifs :
1. **ğŸ” DÃ©tection des Anomalies** : Identifier en temps rÃ©el les anomalies de consommation et les cyberattaques potentielles.
2. **ğŸ“ˆ PrÃ©diction des Incidents** : Utiliser des modÃ¨les prÃ©dictifs pour anticiper les risques futurs.
3. **âš¡ Optimisation des Ressources** : Aider les entreprises Ã  optimiser leur consommation Ã©nergÃ©tique.
4. **ğŸ”„ AmÃ©lioration Continue** : IntÃ©grer les retours des utilisateurs pour perfectionner notre systÃ¨me.
5. **ğŸŒ RÃ©duction de l'Empreinte Carbone** : Calculer l'impact carbone de nos serveurs et de nos traitements de donnÃ©es.
6. **ğŸ“… PrÃ©vision et Planification** : Utiliser des sÃ©ries temporelles pour prÃ©dire les besoins futurs en Ã©nergie.

## 4. ğŸ—ï¸ Architecture du Projet
Le projet est divisÃ© en plusieurs modules interconnectÃ©s, chacun jouant un rÃ´le clÃ© dans l'ensemble du systÃ¨me.

### Workflow GÃ©nÃ©ral
![Workflow](images/Workflow.png)

### ğŸ› ï¸ Module Collecte et Stockage des DonnÃ©es

### ğŸ› ï¸ Module Traitement, Stockage et Visualisation

### ğŸ› ï¸ Module EntraÃ®nement et Suivi des ModÃ¨les

### ğŸ› ï¸ Module DÃ©ploiement et Feedback

### ğŸ› ï¸ Module d'Orchestration et Conteneurisation

## 5. âš™ï¸ IntÃ©gration Continue (CI) et Tests Unitaires
Nous avons mis en place une CI via GitHub Actions, qui exÃ©cute des tests unitaires pour chaque sous-projet Ã  chaque commit.

### Outils utilisÃ©s pour la CI :
- **ğŸ§ª Pytest** pour les tests unitaires
- **ğŸ” Pylint, Black, Mypy** pour l'analyse statique et le formatage du code
- **ğŸ“Š Coverage** pour mesurer la couverture des tests

Le pipeline de CI est disponible dans le rÃ©pertoire `.github/workflows`.

Chaque module du projet est containerisÃ© avec Docker pour garantir la portabilitÃ© et la cohÃ©rence des environnements. Les fichiers `.env` sont utilisÃ©s pour configurer les variables d'environnement de maniÃ¨re flexible.

## 6. ğŸ“‚ Structure du Projet
(InsÃ©rer la structure dÃ©taillÃ©e du projet ici)

Nous utilisons un **ğŸ› ï¸ Makefile** pour automatiser les processus de build, de tests et faciliter la gestion de la CI locale.

## 7. ğŸ”„ Pipelines de Collecte de DonnÃ©es avec Kedro
Deux pipelines Kedro ont Ã©tÃ© mis en place :
1. **Pipeline ETL** : Ce pipeline collecte, transforme et stocke les donnÃ©es dans MongoDB.
2. **Pipeline d'Enrichissement** : Ce pipeline charge les donnÃ©es, les fusionne et les stocke dans Elasticsearch.

Les donnÃ©es brutes stockÃ©es dans Elasticsearch sont visualisÃ©es dans un tableau de bord **Kibana** hÃ©bergÃ© sur une machine virtuelle **GCP**. Voici une capture d'Ã©cran du dashboard Kibana :

![Capture du Dashboard Kibana](lien_capture_kibana)

Pour plus de dÃ©tails, consultez le [README de la partie Kedro](lien_readme_kedro).

## 8. ğŸ’» Traitement des DonnÃ©es et Utilisation de Google Colab
Certaines donnÃ©es volumineuses ont Ã©tÃ© traitÃ©es avec **Google Colab**, notamment pour les membres de l'Ã©quipe ayant des limitations matÃ©rielles. Voici une capture d'Ã©cran de nos notebooks sur Google Colab ainsi que notre bucket GCP pour le stockage des donnÃ©es et artefacts. Nous utilisons MLflow pour le tracking de nos modÃ¨les :

![Capture du Bucket GCP](lien_capture_bucket)

## 9. ğŸ¤– ModÃ¨les de Machine Learning
Les donnÃ©es ont Ã©tÃ© divisÃ©es en deux groupes :
1. **Consommation journaliÃ¨re par rÃ©gion avec donnÃ©es mÃ©tÃ©orologiques**.
2. **Consommation journaliÃ¨re et mouvements sociaux** (avec une colonne "mouvement social" indiquant les jours avec des Ã©vÃ©nements).

Ces deux groupes de donnÃ©es ont conduit Ã  deux sous-projets distincts :
- [Sous-projet sur la consommation rÃ©gionale et les donnÃ©es mÃ©tÃ©o](lien_readme_conso_meteo).
- [Sous-projet sur la consommation et les mouvements sociaux](lien_readme_conso_social).

Ces sous-projets, ainsi que notre interface Streamlit, utilisent **MLflow** pour le suivi et la mise en production des modÃ¨les. Un serveur **MLFlow** a Ã©tÃ© dÃ©ployÃ© sur une VM GCP pour permettre Ã  l'Ã©quipe de suivre les performances des modÃ¨les.

## 10. ğŸ–¥ï¸ Interface Utilisateur avec Streamlit
L'interface utilisateur finale a Ã©tÃ© dÃ©veloppÃ©e avec **Streamlit**. Elle permet :
- Le tÃ©lÃ©chargement de datasets.
- La visualisation des rÃ©sultats des modÃ¨les de machine learning.
- La collecte de feedbacks utilisateurs.

Cette interface est dÃ©ployÃ©e localement et sur **Streamlit Community**. Pour plus de dÃ©tails, voir le [README de lâ€™interface Streamlit](lien_readme_streamlit).

## 11. ğŸ“Š Ordonnancement des DonnÃ©es avec Airflow
Nous avons documentÃ© plusieurs Ã©tapes critiques du projet :
1. **Mise en place dâ€™un serveur MLFlow sur GCP** : [lien_documentation_mlflow]
2. **Mise en place dâ€™un serveur Airflow en local** : [lien_documentation_airflow]
3. **Ordonnancement des DonnÃ©es avec Airflow** : [lien_documentation_airflow]

**Airflow** est utilisÃ© pour orchestrer les pipelines de collecte de donnÃ©es via des DAGs. Un exemple de DAG est utilisÃ© pour enrichir nos datasets avec des donnÃ©es d'API. Ce script Airflow s'exÃ©cute chaque jour Ã  20h pour une durÃ©e de 30 minutes. Voici des images de notre DAG et de notre interface Airflow :
