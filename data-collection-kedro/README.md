# **ğŸš€ Data-collection-kedro - Projet Kedro**

## **Table des matiÃ¨res** ğŸ“š
1. [Vue d'ensemble du projet](#vue-densemble-du-projet)
2. [Architecture du projet](#architecture-du-projet)
3. [Installation et configuration](#installation-et-configuration)
4. [Structure du projet](#structure-du-projet)
5. [ExÃ©cution du projet](#exÃ©cution-du-projet)
6. [Description des pipelines](#description-des-pipelines)
7. [Fichiers de configuration](#fichiers-de-configuration)
8. [Tests du projet](#tests-du-projet)
9. [Contribution](#contribution)
10. [Licence](#licence)


## **Vue d'ensemble du projet** ğŸŒ

`Data-collection-kedro` est un projet de pipeline de donnÃ©es construit autour du framework Kedro, utilisÃ© pour automatiser les processus d'extraction, de transformation et de chargement (ETL). Il inclut des fonctionnalitÃ©s de dÃ©tection d'anomalies dans des donnÃ©es temporelles et catÃ©goriques, avec stockage dans MongoDB et Elasticsearch.

Le projet se concentre sur l'intÃ©gration de donnÃ©es provenant de diverses sources (API, fichiers CSV, XML), leur stockage et leur fusion dans des bases de donnÃ©es.


## **Architecture du projet** ğŸ—ï¸

Le projet suit une architecture modulaire basÃ©e sur Kedro, oÃ¹ chaque tÃ¢che de traitement de donnÃ©es est encapsulÃ©e dans des pipelines distincts pour favoriser la flexibilitÃ© et la maintenance.

### **Vue d'ensemble des pipelines :**

- **Pipeline ETL (`etl_pipeline`)** : Extraction, transformation et stockage des donnÃ©es dans MongoDB.
- **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** : Fusion et stockage des donnÃ©es dans Elasticsearch.


## **Installation et configuration** âš™ï¸

### **PrÃ©requis :**

- **Python 3.8** ou version plus rÃ©cente
- **MongoDB** (cloud ou instance locale)
- **Elasticsearch** (cloud ou instance locale)
- **Docker** (optionnel pour containeriser le projet)

### **Ã‰tapes d'installation :**

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/votreutilisateur/detectionanomalie.git
   cd detectionanomalie
   ```

2. **CrÃ©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les dÃ©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer les variables d'environnement :**
   CrÃ©ez un fichier `.env` et renseignez les informations de connexion MongoDB et Elasticsearch :
   ```env
   MONGODB_USERNAME=nom_utilisateur_mongo
   MONGODB_PASSWORD=mot_de_passe_mongo
   MONGODB_CLUSTER=adresse_du_cluster_mongo

   ELASTIC_USERNAME=nom_utilisateur_elastic
   ELASTIC_PASSWORD=mot_de_passe_elastic
   ELASTIC_DEPLOYMENT_ENDPOINT=adresse_du_cluster_elastic
   ```


## **Structure du projet** ğŸ—‚ï¸

La structure du projet suit les conventions de Kedro :

```
detectionanomalie/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 01_raw/            # DonnÃ©es brutes
â”‚   â”œâ”€â”€ 02_intermediate/    # DonnÃ©es intermÃ©diaires
â”‚   â”œâ”€â”€ 03_primary/         # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ README.md           # Documentation sur les donnÃ©es
â”‚
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ base/               # Configuration commune Ã  tous les environnements
â”‚   â””â”€â”€ local/              # Configuration spÃ©cifique Ã  l'environnement local
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection_kedro/
â”‚   â”‚   â”œâ”€â”€ pipelines/      # DÃ©finition des pipelines
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/                  # Tests unitaires
â”‚
â”œâ”€â”€ Dockerfile              # Dockerfile pour containeriser le projet
â”œâ”€â”€ pyproject.toml          # MÃ©tadonnÃ©es et dÃ©pendances du projet
â””â”€â”€ README.md               # Documentation du projet
```


## **ExÃ©cution du projet** ğŸš€

### **ExÃ©cuter localement :**

- **ExÃ©cuter tous les pipelines :**
   ```bash
   kedro run
   ```

- **ExÃ©cuter un pipeline spÃ©cifique :**
   ```bash
   kedro run --pipeline=etl_pipeline
   ```

### **ExÃ©cuter avec Docker :**

- **Construire l'image Docker :**
   ```bash
   docker build -t kedro-data-engineering .
   ```

- **ExÃ©cuter le conteneur Docker :**
   ```bash
   docker run -it kedro-data-engineering
   ```


## **Description des pipelines** ğŸ”„

### **Pipeline ETL (`etl_pipeline`)** ğŸ› ï¸

- **Objectif** : Extraire des donnÃ©es API/CSV, les transformer et les stocker dans MongoDB.
- **Fonctions principales** :
  - `fetch_data_from_api()`
  - `read_csv_file()`
  - `store_in_mongodb()`

### **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** ğŸ”—

- **Objectif** : Fusionner plusieurs jeux de donnÃ©es, normaliser les colonnes et les stocker dans Elasticsearch.
- **Fonctions principales** :
  - `load_collections()`
  - `select_columns()`
  - `normalize_columns()`
  - `merge_data_store_in_elastic()`


## **Fichiers de configuration** ğŸ› ï¸

### **1. `catalog.yml`** :
- DÃ©finit les jeux de donnÃ©es, leurs sources et destinations (MongoDB, Elasticsearch).

### **2. `parameters.yml`** :
- Contient les paramÃ¨tres globaux comme la taille des chunks ou les URL des API.


## **Tests du projet** ğŸ§ª

Les tests sont rÃ©alisÃ©s avec **pytest**. Les tests unitaires sont disponibles dans le rÃ©pertoire `tests/`.

### **ExÃ©cuter les tests :**

- **Tous les tests** :
   ```bash
   pytest
   ```

- **Tester un pipeline spÃ©cifique** :
   ```bash
   pytest tests/pipelines/etl_pipeline/
   ```


## **Exemples d'images** ğŸ–¼ï¸

Vous pouvez inclure des captures d'Ã©cran des exÃ©cutions de vos pipelines, ainsi que des rÃ©sultats de tests ou du coverage :

### **Exemple d'image - ExÃ©cution du pipeline ETL :**

```markdown
![Pipeline ETL](./images/pipeline_etl_execution.png)
```

### **Exemple d'image - ExÃ©cution du pipeline de fusion :**

```markdown
![Pipeline de fusion](./images/pipeline_fusion_execution.png)
```

### **Exemple d'image - Tests unitaires et couverture :**

```markdown
![Tests unitaires](./images/test_execution.png)
![Coverage des tests](./images/coverage.png)
```

### **Exemple d'image - Visualisation MongoDB :**

Vous pouvez Ã©galement ajouter une capture de la base de donnÃ©es MongoDB :

```markdown
![MongoDB](./images/mongodb.png)
```

### **Exemple d'image - Visualisation Elasticsearch :**

De la mÃªme maniÃ¨re, ajoutez une capture pour Elasticsearch :

```markdown
![Elasticsearch](./images/elasticsearch.png)
```


