# **ğŸš€ Projet Kedro : Mise en place des pipelines de donnÃ©es**

### Langage

![Python](https://img.shields.io/badge/Python-3.11.5-blue?style=for-the-badge&logo=python)

### Frameworks et Outils de DÃ©veloppement

![Kedro](https://img.shields.io/badge/Kedro-0.19.8-green?style=for-the-badge&logo=kedro)
![Dynaconf](https://img.shields.io/badge/Dynaconf-3.2.6-yellow?style=for-the-badge&logo=python)
![Jinja2](https://img.shields.io/badge/Jinja2-3.1.4-red?style=for-the-badge&logo=jinja2)
![Pre-commit Hooks](https://img.shields.io/badge/Pre--commit--hooks-4.6.0-blue?style=for-the-badge&logo=pre-commit)
![GitPython](https://img.shields.io/badge/GitPython-3.1.43-orange?style=for-the-badge&logo=git)
![Cookiecutter](https://img.shields.io/badge/Cookiecutter-2.6.0-green?style=for-the-badge&logo=cookiecutter)

### Empreinte Carbone

![CodeCarbon](https://img.shields.io/badge/CODECARBON-v1.2.0-brightgreen)


### Cloud & Bases de DonnÃ©es

![Elasticsearch](https://img.shields.io/badge/Elasticsearch-8.15.0-005571?style=for-the-badge&logo=elasticsearch)
![Pymongo](https://img.shields.io/badge/Pymongo-4.8.0-darkgreen?style=for-the-badge&logo=mongodb)
![Dynaconf](https://img.shields.io/badge/Dynaconf-3.2.6-orange?style=for-the-badge&logo=python)

### BibliothÃ¨ques de DonnÃ©es & Machine Learning

![Pandas](https://img.shields.io/badge/Pandas-2.2.2-green?style=for-the-badge&logo=pandas)
![Numpy](https://img.shields.io/badge/Numpy-2.1.0-blue?style=for-the-badge&logo=numpy)
![Fsspec](https://img.shields.io/badge/Fsspec-2024.6.1-lightblue?style=for-the-badge&logo=python)
![Matplotlib](https://img.shields.io/badge/Matplotlib--inline-0.1.7-blue?style=for-the-badge&logo=python)

### Outils de DÃ©bogage et de Terminal

![IPython](https://img.shields.io/badge/IPython-8.27.0-lightgrey?style=for-the-badge&logo=ipython)
![Rich](https://img.shields.io/badge/Rich-13.8.0-blue?style=for-the-badge&logo=rich)
![Pygments](https://img.shields.io/badge/Pygments-2.18.0-yellow?style=for-the-badge&logo=python)
![Prompt_toolkit](https://img.shields.io/badge/Prompt--Toolkit-3.0.47-lightgrey?style=for-the-badge&logo=python)


## **Table des matiÃ¨res** ğŸ“š
1. [ğŸŒ Vue d'ensemble du projet](#vue-densemble-du-projet)
2. [ğŸ—ï¸ Architecture du projet](#architecture-du-projet)
3. [âš™ï¸ Installation et configuration](#installation-et-configuration)
4. [ğŸ—‚ï¸ Structure du projet](#structure-du-projet)
5. [ğŸŒ Empreinte Carbone](#empreinte_carbone)
6. [ğŸš€ ExÃ©cution du projet](#exÃ©cution-du-projet)
7. [ğŸ”„ Description des pipelines](#description-des-pipelines)
8. [ğŸ“Š Visualisation des donnÃ©es brutes collectÃ©es](#visualisation-des-donnÃ©es-brutes-collectÃ©es)
9. [ğŸ› ï¸ Fichiers de configuration](#fichiers-de-configuration)
10. [ğŸ§ª Tests du projet](#tests-du-projet)
11. [ğŸ–¼ï¸ Exemples d'images](#exemples-dimages)


## **Vue d'ensemble du projet** ğŸŒ <a name="vue-densemble-du-projet"></a>

`Data-collection-kedro` est un projet de pipeline de donnÃ©es construit autour du framework Kedro, utilisÃ© pour automatiser les processus d'extraction, de transformation et de chargement (ETL). Il est inclure dans notre projet de dÃ©tection d'anomalies dans des donnÃ©es temporelles et catÃ©goriques, avec stockage des donnÃ©es dans MongoDB et Elasticsearch.

Le projet se concentre sur l'intÃ©gration de donnÃ©es provenant de diverses sources (API, fichiers CSV, XML), leur stockage et leur fusion dans des bases de donnÃ©es.
**Une fonctionnalitÃ© clÃ©** du projet est le **suivi de l'empreinte carbone** des opÃ©rations de traitement, rÃ©alisÃ© Ã  l'aide de la bibliothÃ¨que **CodeCarbon**. Cela permet de mesurer l'impact environnemental des pipelines de donnÃ©es Ã  chaque exÃ©cution, contribuant ainsi Ã  un dÃ©veloppement plus durable.

## **Architecture du projet** ğŸ—ï¸ <a name="architecture-du-projet"></a>

Ce sous-projet suit une architecture modulaire basÃ©e sur Kedro, oÃ¹ chaque tÃ¢che de traitement de donnÃ©es est encapsulÃ©e dans des pipelines distincts pour favoriser la flexibilitÃ© et la maintenance.


### **Vue d'ensemble des pipelines :**

- **Pipeline ETL (`etl_pipeline`)** : Extraction, transformation et stockage des donnÃ©es dans MongoDB.
- **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** : Fusion et stockage des donnÃ©es dans Elasticsearch.

## **Installation et configuration** âš™ï¸ <a name="installation-et-configuration"></a>

### **PrÃ©requis :**

- **Python 3.8** ou version plus rÃ©cente
- **MongoDB** (cloud ou instance locale)
- **Elasticsearch** (cloud ou instance locale)
- **Docker** (optionnel pour containeriser le projet)

### **Ã‰tapes d'installation :**

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/keagnon/DetectionAnomalie.git
   cd DetectionAnomalie
   ```

2. **CrÃ©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les dÃ©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer les variables d'environnement :**
   CrÃ©ez un fichier `.env` et renseignez les informations de connexion MongoDB et Elasticsearch :
   ```env
   MONGODB_USERNAME=nom_utilisateur_mongo
   MONGODB_PASSWORD=mot_de_passe_mongo
   MONGODB_CLUSTER=adresse_du_cluster_mongo
   MONGODB_DBNAME=nom_base_de_donnÃ©e

   ELASTIC_USERNAME=nom_utilisateur_elastic
   ELASTIC_PASSWORD=mot_de_passe_elastic
   ELASTIC_DEPLOYMENT_ENDPOINT=adresse_du_cluster_elastic
   ```

## **Structure du projet** ğŸ—‚ï¸ <a name="structure-du-projet"></a>

La structure du projet suit les conventions de Kedro :

```
data-collection-kedro/
â”‚
â”œâ”€â”€ conf/                                # Fichiers de configuration
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ catalog.yml                  # DÃ©finition des jeux de donnÃ©es et leurs emplacements
â”‚   â”‚   â”œâ”€â”€ parameters_data_fusion_pipeline.yml
â”‚   â”‚   â”œâ”€â”€ parameters_etl_pipeline.yml
â”‚   â”‚   â””â”€â”€ parameters.yml               # ParamÃ¨tres globaux du projet
â”‚   â””â”€â”€ local/                           # Configuration spÃ©cifique Ã  l'environnement local
â”‚
â”œâ”€â”€ data/                                # DonnÃ©es utilisÃ©es dans le projet
â”‚   â”œâ”€â”€ data_carburant_xml/
â”‚   â”œâ”€â”€ data_merge/
â”‚   â”œâ”€â”€ meteo/
â”‚   â”‚   â”œâ”€â”€ combined_meteo_data.csv
â”‚   â”‚   â”œâ”€â”€ courbe_de_charge_data_2018_2024.csv
â”‚   â”‚   â”œâ”€â”€ meteo_data_2018_2024.csv
â”‚   â”‚   â”œâ”€â”€ movement_data_2018_2024.csv
â”‚   â”‚   â””â”€â”€ prix_du_carburant_data_2018_2024.csv
â”‚   â””â”€â”€ README.md                       # Documentation sur les donnÃ©es
â”‚
â”œâ”€â”€ images/                             # Captures d'Ã©cran
â”‚   â”œâ”€â”€ data_fusion/
â”‚   â”œâ”€â”€ elastic_search/
â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â””â”€â”€ tests_unitaires/
â”‚
â”œâ”€â”€ logs/                               # Dossier de logs pour l'empreinte carbone et autres logs
â”‚   â”œâ”€â”€ log_data_fusion_pipeline/       # Logs gÃ©nÃ©rÃ©s par CodeCarbon lors de l'exÃ©cution de la pipeline data fusion pipeline
â”‚   â”‚   â”œâ”€â”€ emissions.csv               # Fichier CSV contenant les Ã©missions de carbone
â”‚   â”œâ”€â”€ logs_etl_pipeline/              # Logs gÃ©nÃ©rÃ©s par CodeCarbon lors de l'exÃ©cution de la pipeline ETL pipeline
â”‚   â”‚   â”œâ”€â”€ emissions.csv               # Fichier CSV contenant les Ã©missions de carbone
â”‚
â”œâ”€â”€ kedro-dataeng-env/                   # Environnement virtuel Kedro
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection_kedro/           # RÃ©pertoire principal des sources du projet
â”‚   â”‚   â”œâ”€â”€ pipelines/                   # Pipelines de traitement des donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ data_fusion_pipeline/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py             # Fonctions spÃ©cifiques au pipeline de fusion de donnÃ©es
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py          # DÃ©finition des pipelines
â”‚   â”‚   â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py             # Fonctions spÃ©cifiques au pipeline ETL
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py          # DÃ©finition de la pipeline ETL
â”‚   â”‚   â””â”€â”€ pipeline_registry.py         # Enregistrement des pipelines Kedro
â”‚
â”œâ”€â”€ tests/                               # Tests unitaires pour le projet
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ data_fusion_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_pipeline.py         # Tests pour le pipeline de fusion de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_pipeline.py         # Tests pour le pipeline ETL
â”‚   â”‚   â”‚   â”œâ”€â”€ test_transform.py        # Tests pour les transformations de donnÃ©es ETL
â”‚
â”œâ”€â”€ Dockerfile                           # Fichier Docker pour containeriser le projet
â”œâ”€â”€ pyproject.toml                       # Fichier de configuration du projet et des dÃ©pendances
â”œâ”€â”€ README.md                            # Documentation principale du projet
â””â”€â”€ requirements.txt                     # Liste des dÃ©pendances Python du projet

```

## **Empreinte Carbone** ğŸŒ  <a name="empreinte_carbone"></a>

Pour ce projet de collecte de donnÃ©es avec Kedro, nous avons utilisÃ© la bibliothÃ¨que **CodeCarbon** pour suivre l'empreinte carbone des pipelines, comme **data_fusion** et **etl_pipeline**. Les rÃ©sultats sont stockÃ©s dans le dossier **logs**, offrant une vue dÃ©taillÃ©e des Ã©missions de CO2eq gÃ©nÃ©rÃ©es par chaque traitement.

### RÃ©sultats de l'Empreinte Carbone :

![Empreinte Carbone dans le Terminal](images/carbon_logs/etl_pipeline/im3.png)
![Empreinte Carbone dans le Terminal](images/carbon_logs/data_fusion/data_fusion_pipeline_co2_3.png)

## **ExÃ©cution du projet** ğŸš€ <a name="exÃ©cution-du-projet"></a>

### **ExÃ©cuter localement :**

- **ExÃ©cuter tous les pipelines :**
   ```bash
   kedro run
   ```

- **ExÃ©cuter un pipeline spÃ©cifique :**
   - **Pipeline ETL** :
     ```bash
     kedro run --pipeline=etl_pipeline
     ```

   - **Pipeline de Fusion de DonnÃ©es** :
     ```bash
     kedro run --pipeline=data_fusion_pipeline
     ```

---

### **ExÃ©cuter avec Docker :**

- **Construire l'image Docker :**
   ```bash
   docker build -t kedro-data-engineering .
   ```

- **ExÃ©cuter le conteneur Docker :**
   ```bash
   docker run -it kedro-data-engineering
   ```


## **Description des pipelines** ğŸ”„ <a name="description-des-pipelines"></a>

### **Pipeline ETL (`etl_pipeline`)** ğŸ› ï¸

- **Objectif** : Extraire des donnÃ©es API/CSV/XML, les transformer et les stocker dans MongoDB.
- **Fonctions principales** :
  - `fetch_data_from_api()`
  - `read_csv_file()`
  - `store_in_mongodb()`

### **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** ğŸ”—

- **Objectif** : Fusionner plusieurs jeux de donnÃ©es, normaliser les colonnes et les stocker dans Elasticsearch.
- **Fonctions principales** :
  - `load_collections()`
  - `select_columns()`
  - `normalize_columns()`
  - `merge_data_store_in_elastic()`


## **Visualisation des donnÃ©es brutes collectÃ©es** ğŸ”„ <a name="visualisation-des-donnÃ©es-brutes-collectÃ©es"></a>

Les donnÃ©es brutes stockÃ©es dans Elasticsearch sont visualisÃ©es dans un tableau de bord **Kibana** hÃ©bergÃ© sur une machine virtuelle **GCP**. Voici une capture d'Ã©cran du dashboard Kibana :

![Capture du Dashboard Kibana](images/dashboard_kibana/donnee_brute_kibana.png)

Les trois premiers graphiques montrent comment la consommation d'Ã©nergie varie en fonction des rÃ©gions et des saisons, influencÃ©e par les conditions mÃ©tÃ©orologiques, tandis que les deux derniers mettent en lumiÃ¨re l'impact des mouvements sociaux sur la baisse de la consommation Ã©nergÃ©tique.

### 1. **Consommation Ã©nergÃ©tique par rÃ©gion (2018-2024)** (Graphique en haut Ã  gauche)
   Ce graphique reprÃ©sente l'Ã©volution de la **consommation Ã©nergÃ©tique cumulÃ©e par rÃ©gion** sur la pÃ©riode 2018-2024. Les pics de consommation semblent Ãªtre saisonniers, avec des hausses notables en hiver, ce qui peut indiquer une corrÃ©lation avec les basses tempÃ©ratures et une demande accrue de chauffage. On observe Ã©galement une lÃ©gÃ¨re baisse vers 2021, ce qui pourrait coÃ¯ncider avec une rÃ©duction d'activitÃ© Ã©conomique, peut-Ãªtre liÃ©e aux restrictions sanitaires.

### 2. **RÃ©partition de la consommation Ã©nergÃ©tique par rÃ©gion** (Diagramme circulaire au centre)
   Ce diagramme montre la part de la **consommation Ã©nergÃ©tique par rÃ©gion**. La **rÃ©gion Ãle-de-France** domine avec environ **30,88 %** de la consommation totale, suivie de **Auvergne-RhÃ´ne-Alpes** (18,47 %) et des **Hauts-de-France** (17,4 %). Cela reflÃ¨te les diffÃ©rences rÃ©gionales en termes de densitÃ© de population et d'activitÃ© Ã©conomique.

### 3. **Ã‰volution de la consommation Ã©nergÃ©tique et des tempÃ©ratures minimales** (Graphique Ã  droite)
   Ce tableau liste les **dates et les tempÃ©ratures minimales** par opÃ©rateur avec les moyennes de consommation journaliÃ¨re. Il permet de comparer l'impact des variations mÃ©tÃ©orologiques (tempÃ©ratures basses) sur la consommation d'Ã©nergie, mais les colonnes de tempÃ©rature semblent vides ici, ce qui pourrait indiquer des donnÃ©es manquantes ou non disponibles Ã  cette Ã©tape.

### 4. **Graphique comparatif entre les jours avec et sans mouvement social** (Graphique en bas Ã  gauche)
   Ce graphique illustre l'impact des **mouvements sociaux** sur la consommation Ã©nergÃ©tique. On observe que les jours avec des mouvements sociaux (barres bleues) entraÃ®nent une baisse notable de la consommation d'Ã©nergie dans plusieurs rÃ©gions. Cela suggÃ¨re une perturbation des activitÃ©s Ã©conomiques, probablement due Ã  des grÃ¨ves ou des manifestations.

### 5. **SÃ©rie temporelle des consommations journaliÃ¨res et horaires** (Graphique en bas Ã  droite)
   Ce graphique montre une **sÃ©rie temporelle** comparant les jours avec mouvements sociaux (en bleu foncÃ©) et sans mouvements sociaux (en vert). La tendance gÃ©nÃ©rale indique que la consommation Ã©nergÃ©tique est plus faible pendant les pÃ©riodes de mouvements sociaux, avec des pics rÃ©guliers sans ces mouvements. Cela confirme l'impact des perturbations sociales sur la consommation Ã©nergÃ©tique.

## **Fichiers de configuration** ğŸ› ï¸ <a name="fichiers-de-configuration"></a>

### **1. `parameters_data_fusion_pipeline.yml`** :
- DÃ©finit les jeux de donnÃ©es, leurs sources et destinations (MongoDB, Elasticsearch).

### **2. `parameters_etl_pipeline.yml`** :
- Contient les paramÃ¨tres globaux comme la taille des chunks ou les URL des API.


## **Tests du projet** ğŸ§ª <a name="tests-du-projet"></a>

Les tests sont rÃ©alisÃ©s avec **pytest**. Les tests unitaires sont disponibles dans le rÃ©pertoire `tests/`.

### **ExÃ©cuter les tests :**

- **Tous les tests** :
   ```bash
   pytest tests/
   ```

- **Tester un pipeline spÃ©cifique** :
   ```bash
   pytest tests/pipelines/etl_pipeline/
   ```

## **Exemples d'images** ğŸ–¼ï¸ <a name="exemples-dimages"></a>

Ici vous trouverez les captures d'Ã©cran des exÃ©cutions de nos pipelines, ainsi que des rÃ©sultats de tests ou du coverage.


### **Exemple d'image - Tests unitaires et couverture :**

![Tests unitaires](images/tests_unitaires/im1.png)


### **Exemple d'image - Visualisation MongoDB :**

![MongoDB](images/mongodb/im1.png)
![MongoDB](images/mongodb/im2.png)
![MongoDB](images/mongodb/im3.png)
![MongoDB](images/mongodb/im4.png)

### **Exemple d'image - ExÃ©cution du pipeline ETL :**

![Pipeline ETL](images/etl_pipeline/im1.png)
![Pipeline ETL](images/etl_pipeline/im2.png)
![Pipeline ETL](images/etl_pipeline/im3.png)
![Pipeline ETL](images/etl_pipeline/im4.png)
![Pipeline ETL](images/etl_pipeline/im5.png)


### **Exemple d'image - Visualisation Elasticsearch :**

![Elasticsearch](images/elastic_search/im1.png)
![Elasticsearch](images/elastic_search/im2.png)
![Elasticsearch](images/elastic_search/im4.png)
![Elasticsearch](images/elastic_search/im5.png)
![Elasticsearch](images/elastic_search/im7.png)

### **Exemple d'image - ExÃ©cution du pipeline de fusion :**

![Pipeline de fusion](images/data_fusion/im1.png)
![Pipeline de fusion](images/data_fusion/im2.png)
![Pipeline de fusion](images/data_fusion/im3.png)
![Pipeline de fusion](images/data_fusion/im4.png)
![Pipeline de fusion](images/data_fusion/im6.png)

### **Exemple d'image - Empreinte carbone tracking pipeline ETL:**

![Empreinte carbone](images/carbon_logs/data_fusion/data_fusion_pipeline_co2_1.png)
![Empreinte carbone](images/carbon_logs/data_fusion/data_fusion_pipeline_co2_2.png)
![Empreinte carbone](images/carbon_logs/data_fusion/data_fusion_pipeline_co2_3.png)

### **Exemple d'image - Empreinte carbone tracking pipeline Data fusion:**

![Empreinte carbone](images/carbon_logs/etl_pipeline/im1.png)
![Empreinte carbone](images/carbon_logs/etl_pipeline/im2.png)
![Empreinte carbone](images/carbon_logs/etl_pipeline/im3.png)
![Empreinte carbone](images/data_fusion/im7.png)
