# **ğŸš€ Data-collection-kedro - Projet Kedro**

## **Table des matiÃ¨res** ğŸ“š
1. [Vue d'ensemble du projet](#vue-densemble-du-projet)
2. [Architecture du projet](#architecture-du-projet)
3. [Installation et configuration](#installation-et-configuration)
4. [Structure du projet](#structure-du-projet)
5. [ExÃ©cution du projet](#exÃ©cution-du-projet)
6. [Description des pipelines](#description-des-pipelines)
7. [Fichiers de configuration](#fichiers-de-configuration)
8. [Tests du projet](#tests-du-projet)
9. [Exemples d'images](#exemples-dimages)


---

## **Vue d'ensemble du projet** ğŸŒ <a name="vue-densemble-du-projet"></a>

`Data-collection-kedro` est un projet de pipeline de donnÃ©es construit autour du framework Kedro, utilisÃ© pour automatiser les processus d'extraction, de transformation et de chargement (ETL). Il est inclure dans notre projet de dÃ©tection d'anomalies dans des donnÃ©es temporelles et catÃ©goriques, avec stockage des donnÃ©es dans MongoDB et Elasticsearch.

Le projet se concentre sur l'intÃ©gration de donnÃ©es provenant de diverses sources (API, fichiers CSV, XML), leur stockage et leur fusion dans des bases de donnÃ©es.

---

## **Architecture du projet** ğŸ—ï¸ <a name="architecture-du-projet"></a>

Le projet suit une architecture modulaire basÃ©e sur Kedro, oÃ¹ chaque tÃ¢che de traitement de donnÃ©es est encapsulÃ©e dans des pipelines distincts pour favoriser la flexibilitÃ© et la maintenance.

### **Vue d'ensemble des pipelines :**

- **Pipeline ETL (`etl_pipeline`)** : Extraction, transformation et stockage des donnÃ©es dans MongoDB.
- **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** : Fusion et stockage des donnÃ©es dans Elasticsearch.

---

## **Installation et configuration** âš™ï¸ <a name="installation-et-configuration"></a>

### **PrÃ©requis :**

- **Python 3.8** ou version plus rÃ©cente
- **MongoDB** (cloud ou instance locale)
- **Elasticsearch** (cloud ou instance locale)
- **Docker** (optionnel pour containeriser le projet)

### **Ã‰tapes d'installation :**

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/keagnon/DetectionAnomalie.git
   cd DetectionAnomalie
   ```

2. **CrÃ©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les dÃ©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer les variables d'environnement :**
   CrÃ©ez un fichier `.env` et renseignez les informations de connexion MongoDB et Elasticsearch :
   ```env
   MONGODB_USERNAME=nom_utilisateur_mongo
   MONGODB_PASSWORD=mot_de_passe_mongo
   MONGODB_CLUSTER=adresse_du_cluster_mongo
   MONGODB_DBNAME=nom_base_de_donnÃ©e

   ELASTIC_USERNAME=nom_utilisateur_elastic
   ELASTIC_PASSWORD=mot_de_passe_elastic
   ELASTIC_DEPLOYMENT_ENDPOINT=adresse_du_cluster_elastic

   ```

---

## **Structure du projet** ğŸ—‚ï¸ <a name="structure-du-projet"></a>

La structure du projet suit les conventions de Kedro :

```
data-collection-kedro/
â”‚
â”œâ”€â”€ conf/                                # Fichiers de configuration
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ catalog.yml                  # DÃ©finition des jeux de donnÃ©es et leurs emplacements
â”‚   â”‚   â”œâ”€â”€ parameters_data_fusion_pipeline.yml
â”‚   â”‚   â”œâ”€â”€ parameters_etl_pipeline.yml
â”‚   â”‚   â””â”€â”€ parameters.yml               # ParamÃ¨tres globaux du projet
â”‚   â””â”€â”€ local/                           # Configuration spÃ©cifique Ã  l'environnement local
â”‚
â”œâ”€â”€ data/                                # DonnÃ©es utilisÃ©es dans le projet
â”‚   â”œâ”€â”€ data_carburant_xml/
â”‚   â”œâ”€â”€ data_merge/
â”‚   â”œâ”€â”€ meteo/
â”‚   â”‚   â”œâ”€â”€ combined_meteo_data.csv
â”‚   â”‚   â”œâ”€â”€ courbe_de_charge_data_2018_2024.csv
â”‚   â”‚   â”œâ”€â”€ meteo_data_2018_2024.csv
â”‚   â”‚   â”œâ”€â”€ movement_data_2018_2024.csv
â”‚   â”‚   â””â”€â”€ prix_du_carburant_data_2018_2024.csv
â”‚   â””â”€â”€ README.md                       # Documentation sur les donnÃ©es
â”‚
â”œâ”€â”€ kedro-dataeng-env/                   # Environnement virtuel Kedro
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection_kedro/           # RÃ©pertoire principal des sources du projet
â”‚   â”‚   â”œâ”€â”€ pipelines/                   # Pipelines de traitement des donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ data_fusion_pipeline/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py             # Fonctions spÃ©cifiques au pipeline de fusion de donnÃ©es
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py          # DÃ©finition des pipelines
â”‚   â”‚   â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py             # Fonctions spÃ©cifiques au pipeline ETL
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ transform.py         # Transformation des donnÃ©es pour ETL
â”‚   â”‚   â”‚   â””â”€â”€ utils.py                 # Fonctions utilitaires
â”‚   â”‚   â”œâ”€â”€ settings.py                  # ParamÃ¨tres du projet Kedro
â”‚   â””â”€â”€ pipeline_registry.py             # Enregistrement des pipelines Kedro
â”‚
â”œâ”€â”€ tests/                               # Tests unitaires pour le projet
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ data_fusion_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_pipeline.py         # Tests pour le pipeline de fusion de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_pipeline.py         # Tests pour le pipeline ETL
â”‚   â”‚   â”‚   â”œâ”€â”€ test_transform.py        # Tests pour les transformations de donnÃ©es ETL
â”‚
â”œâ”€â”€ Dockerfile                           # Fichier Docker pour containeriser le projet
â”œâ”€â”€ pyproject.toml                       # Fichier de configuration du projet et des dÃ©pendances
â”œâ”€â”€ README.md                            # Documentation principale du projet
â””â”€â”€ requirements.txt                     # Liste des dÃ©pendances Python du projet

```

---

## **ExÃ©cution du projet** ğŸš€ <a name="exÃ©cution-du-projet"></a>

### **ExÃ©cuter localement :**

- **ExÃ©cuter tous les pipelines :**
   ```bash
   kedro run
   ```

- **ExÃ©cuter un pipeline spÃ©cifique :**
   ```bash
   kedro run --pipeline=etl_pipeline
   ```
   ou

   ```bash
   kedro run --pipeline=data_fusion_pipeline
   ```

### **ExÃ©cuter avec Docker :**

- **Construire l'image Docker :**
   ```bash
   docker build -t kedro-data-engineering .
   ```

- **ExÃ©cuter le conteneur Docker :**
   ```bash
   docker run -it kedro-data-engineering
   ```

---

## **Description des pipelines** ğŸ”„ <a name="description-des-pipelines"></a>

### **Pipeline ETL (`etl_pipeline`)** ğŸ› ï¸

- **Objectif** : Extraire des donnÃ©es API/CSV/XML, les transformer et les stocker dans MongoDB.
- **Fonctions principales** :
  - `fetch_data_from_api()`
  - `read_csv_file()`
  - `store_in_mongodb()`

### **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** ğŸ”—

- **Objectif** : Fusionner plusieurs jeux de donnÃ©es, normaliser les colonnes et les stocker dans Elasticsearch.
- **Fonctions principales** :
  - `load_collections()`
  - `select_columns()`
  - `normalize_columns()`
  - `merge_data_store_in_elastic()`

---

## **Fichiers de configuration** ğŸ› ï¸ <a name="fichiers-de-configuration"></a>

### **1. `parameters_data_fusion_pipeline.yml`** :
- DÃ©finit les jeux de donnÃ©es, leurs sources et destinations (MongoDB, Elasticsearch).

### **2. `parameters_etl_pipeline.yml`** :
- Contient les paramÃ¨tres globaux comme la taille des chunks ou les URL des API.

---

## **Tests du projet** ğŸ§ª <a name="tests-du-projet"></a>

Les tests sont rÃ©alisÃ©s avec **pytest**. Les tests unitaires sont disponibles dans le rÃ©pertoire `tests/`.

### **ExÃ©cuter les tests :**

- **Tous les tests** :
   ```bash
   pytest
   ```

- **Tester un pipeline spÃ©cifique** :
   ```bash
   pytest tests/pipelines/etl_pipeline/
   ```

---

## **Exemples d'images** ğŸ–¼ï¸ <a name="exemples-dimages"></a>

Vous pouvez inclure des captures d'Ã©cran des exÃ©cutions de vos pipelines, ainsi que des rÃ©sultats de tests ou du coverage :

### **Exemple d'image - ExÃ©cution du pipeline ETL :**

![Pipeline ETL](images/etl_pipeline/im1.png)

### **Exemple d'image - ExÃ©cution du pipeline de fusion :**

```markdown
![Pipeline de fusion](./images/pipeline_fusion_execution.png)
```

### **Exemple d'image - Tests unitaires et couverture :**

```markdown
![Tests unitaires](./images/test_execution.png)
![Coverage des tests](./images/coverage.png)
```

### **Exemple d'image - Visualisation MongoDB :**

Vous pouvez Ã©galement ajouter une capture de la base de donnÃ©es MongoDB :

```markdown
![MongoDB](./images/mongodb.png)
```

### **Exemple d'image - Visualisation Elasticsearch :**

De la mÃªme maniÃ¨re, ajoutez une capture pour Elasticsearch :

```markdown
![Elasticsearch](./images/elasticsearch.png)
```

