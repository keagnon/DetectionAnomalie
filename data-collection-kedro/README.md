# **üöÄ Data-collection-kedro - Projet Kedro**

## **Table des mati√®res** üìö
1. [Vue d'ensemble du projet](#vue-densemble-du-projet)
2. [Architecture du projet](#architecture-du-projet)
3. [Installation et configuration](#installation-et-configuration)
4. [Structure du projet](#structure-du-projet)
5. [Ex√©cution du projet](#ex√©cution-du-projet)
6. [Description des pipelines](#description-des-pipelines)
7. [Fichiers de configuration](#fichiers-de-configuration)
8. [Tests du projet](#tests-du-projet)
9. [Contribution](#contribution)
10. [Licence](#licence)

---

## **Vue d'ensemble du projet** üåç

data-collection-kedro est un projet de pipeline de donn√©es construit autour du framework Kedro, utilis√© pour automatiser les processus d'extraction, de transformation et de chargement (ETL). Il inclut des fonctionnalit√©s de d√©tection d'anomalies dans des donn√©es temporelles et cat√©goriques, avec stockage dans MongoDB et Elasticsearch.

Le projet se concentre sur l'int√©gration de donn√©es provenant de diverses sources (API, fichiers CSV, XML), leur stockage et leur fusion dans des bases de donn√©es.

---

## **Architecture du projet** üèóÔ∏è

Le projet suit une architecture modulaire bas√©e sur Kedro, o√π chaque t√¢che de traitement de donn√©es est encapsul√©e dans des pipelines distincts pour favoriser la flexibilit√© et la maintenance.

### **Vue d'ensemble des pipelines :**

- **Pipeline ETL (`etl_pipeline`)** : Extraction, transformation et stockage des donn√©es dans MongoDB.
- **Pipeline de Fusion de Donn√©es (`data_fusion_pipeline`)** : Fusion et stockage des donn√©es dans Elasticsearch.

---

## **Installation et configuration** ‚öôÔ∏è

### **Pr√©requis :**

- **Python 3.8** ou version plus r√©cente
- **MongoDB** (cloud ou instance locale)
- **Elasticsearch** (cloud ou instance locale)
- **Docker** (optionnel pour containeriser le projet)

### **√âtapes d'installation :**

1. **Cloner le d√©p√¥t :**
   ```bash
   git clone https://github.com/votreutilisateur/detectionanomalie.git
   cd detectionanomalie
   ```

2. **Cr√©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les d√©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer les variables d'environnement :**
   Cr√©ez un fichier `.env` et renseignez les informations de connexion MongoDB et Elasticsearch :
   ```env
   MONGODB_USERNAME=nom_utilisateur_mongo
   MONGODB_PASSWORD=mot_de_passe_mongo
   MONGODB_CLUSTER=adresse_du_cluster_mongo

   ELASTIC_USERNAME=nom_utilisateur_elastic
   ELASTIC_PASSWORD=mot_de_passe_elastic
   ELASTIC_DEPLOYMENT_ENDPOINT=adresse_du_cluster_elastic
   ```

---

## **Structure du projet** üóÇÔ∏è

La structure du projet suit les conventions de Kedro :

```
detectionanomalie/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ 01_raw/            # Donn√©es brutes
‚îÇ   ‚îú‚îÄ‚îÄ 02_intermediate/    # Donn√©es interm√©diaires
‚îÇ   ‚îú‚îÄ‚îÄ 03_primary/         # Donn√©es nettoy√©es
‚îÇ   ‚îî‚îÄ‚îÄ README.md           # Documentation sur les donn√©es
‚îÇ
‚îú‚îÄ‚îÄ conf/
‚îÇ   ‚îú‚îÄ‚îÄ base/               # Configuration commune √† tous les environnements
‚îÇ   ‚îî‚îÄ‚îÄ local/              # Configuration sp√©cifique √† l'environnement local
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data_collection_kedro/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines/      # D√©finition des pipelines
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ tests/                  # Tests unitaires
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile              # Dockerfile pour containeriser le projet
‚îú‚îÄ‚îÄ pyproject.toml          # M√©tadonn√©es et d√©pendances du projet
‚îî‚îÄ‚îÄ README.md               # Documentation du projet
```

---

## **Ex√©cution du projet** üöÄ

### **Ex√©cuter localement :**

- **Ex√©cuter tous les pipelines :**
   ```bash
   kedro run
   ```

- **Ex√©cuter un pipeline sp√©cifique :**
   ```bash
   kedro run --pipeline=etl_pipeline
   ```

### **Ex√©cuter avec Docker :**

- **Construire l'image Docker :**
   ```bash
   docker build -t kedro-data-engineering .
   ```

- **Ex√©cuter le conteneur Docker :**
   ```bash
   docker run -it kedro-data-engineering
   ```

---

## **Description des pipelines** üîÑ

### **Pipeline ETL (`etl_pipeline`)** üõ†Ô∏è

- **Objectif** : Extraire des donn√©es API/CSV, les transformer et les stocker dans MongoDB.
- **Fonctions principales** :
  - `fetch_data_from_api()`
  - `read_csv_file()`
  - `store_in_mongodb()`

### **Pipeline de Fusion de Donn√©es (`data_fusion_pipeline`)** üîó

- **Objectif** : Fusionner plusieurs jeux de donn√©es, normaliser les colonnes et les stocker dans Elasticsearch.
- **Fonctions principales** :
  - `load_collections()`
  - `select_columns()`
  - `normalize_columns()`
  - `merge_data_store_in_elastic()`

---

## **Fichiers de configuration** üõ†Ô∏è

### **1. `catalog.yml`** :
- D√©finit les jeux de donn√©es, leurs sources et destinations (MongoDB, Elasticsearch).

### **2. `parameters.yml`** :
- Contient les param√®tres globaux comme la taille des chunks ou les URL des API.

---

## **Tests du projet** üß™

Les tests sont r√©alis√©s avec **pytest**. Les tests unitaires sont disponibles dans le r√©pertoire `tests/`.

### **Ex√©cuter les tests :**

- **Tous les tests** :
   ```bash
   pytest
   ```

- **Tester un pipeline sp√©cifique** :
   ```bash
   pytest tests/pipelines/etl_pipeline/
   ```

---

## **Exemples d'images** üñºÔ∏è

Vous pouvez inclure des captures d'√©cran des ex√©cutions de vos pipelines, ainsi que des r√©sultats de tests ou du coverage :

### **Exemple d'image - Ex√©cution du pipeline ETL :**

```markdown
![Pipeline ETL](./images/pipeline_etl_execution.png)
```

### **Exemple d'image - Ex√©cution du pipeline de fusion :**

```markdown
![Pipeline de fusion](./images/pipeline_fusion_execution.png)
```

### **Exemple d'image - Tests unitaires et couverture :**

```markdown
![Tests unitaires](./images/test_execution.png)
![Coverage des tests](./images/coverage.png)
```

### **Exemple d'image - Visualisation MongoDB :**

Vous pouvez √©galement ajouter une capture de la base de donn√©es MongoDB :

```markdown
![MongoDB](./images/mongodb.png)
```

### **Exemple d'image - Visualisation Elasticsearch :**

De la m√™me mani√®re, ajoutez une capture pour Elasticsearch :

```markdown
![Elasticsearch](./images/elasticsearch.png)
```

> Placez les images dans le r√©pertoire `images/` √† la racine du projet.

---

