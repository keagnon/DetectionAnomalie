# **ğŸš€ Data-collection-kedro - Projet Kedro**

### Langage

![Python](https://img.shields.io/badge/Python-3.9-blue?style=for-the-badge&logo=python)

### Frameworks et Outils de DÃ©veloppement

![Kedro](https://img.shields.io/badge/Kedro-0.19.8-green?style=for-the-badge&logo=kedro)
![Dynaconf](https://img.shields.io/badge/Dynaconf-3.2.6-yellow?style=for-the-badge&logo=python)
![Jinja2](https://img.shields.io/badge/Jinja2-3.1.4-red?style=for-the-badge&logo=jinja2)
![Pre-commit Hooks](https://img.shields.io/badge/Pre--commit--hooks-4.6.0-blue?style=for-the-badge&logo=pre-commit)
![GitPython](https://img.shields.io/badge/GitPython-3.1.43-orange?style=for-the-badge&logo=git)
![Cookiecutter](https://img.shields.io/badge/Cookiecutter-2.6.0-green?style=for-the-badge&logo=cookiecutter)

### Empreinte Carbone

![CodeCarbon](https://img.shields.io/badge/CODECARBON-v1.2.0-brightgreen)


### Cloud & Bases de DonnÃ©es

![Elasticsearch](https://img.shields.io/badge/Elasticsearch-8.15.0-005571?style=for-the-badge&logo=elasticsearch)
![Pymongo](https://img.shields.io/badge/Pymongo-4.8.0-darkgreen?style=for-the-badge&logo=mongodb)
![Dynaconf](https://img.shields.io/badge/Dynaconf-3.2.6-orange?style=for-the-badge&logo=python)

### BibliothÃ¨ques de DonnÃ©es & Machine Learning

![Pandas](https://img.shields.io/badge/Pandas-2.2.2-green?style=for-the-badge&logo=pandas)
![Numpy](https://img.shields.io/badge/Numpy-2.1.0-blue?style=for-the-badge&logo=numpy)
![Fsspec](https://img.shields.io/badge/Fsspec-2024.6.1-lightblue?style=for-the-badge&logo=python)
![Matplotlib](https://img.shields.io/badge/Matplotlib--inline-0.1.7-blue?style=for-the-badge&logo=python)

### Outils de DÃ©bogage et de Terminal

![IPython](https://img.shields.io/badge/IPython-8.27.0-lightgrey?style=for-the-badge&logo=ipython)
![Rich](https://img.shields.io/badge/Rich-13.8.0-blue?style=for-the-badge&logo=rich)
![Pygments](https://img.shields.io/badge/Pygments-2.18.0-yellow?style=for-the-badge&logo=python)
![Prompt_toolkit](https://img.shields.io/badge/Prompt--Toolkit-3.0.47-lightgrey?style=for-the-badge&logo=python)


## **Table des matiÃ¨res** ğŸ“š
1. [Vue d'ensemble du projet](#vue-densemble-du-projet)
2. [Architecture du projet](#architecture-du-projet)
3. [Installation et configuration](#installation-et-configuration)
4. [Structure du projet](#structure-du-projet)
5. [ExÃ©cution du projet](#exÃ©cution-du-projet)
6. [Description des pipelines](#description-des-pipelines)
7. [Fichiers de configuration](#fichiers-de-configuration)
8. [Tests du projet](#tests-du-projet)
9. [Empreinte Carbone](#empreinte-carbone)
10. [Exemples d'images](#exemples-dimages)


## **Vue d'ensemble du projet** ğŸŒ <a name="vue-densemble-du-projet"></a>

`Data-collection-kedro` est un projet de pipeline de donnÃ©es construit autour du framework Kedro, utilisÃ© pour automatiser les processus d'extraction, de transformation et de chargement (ETL). Il est inclure dans notre projet de dÃ©tection d'anomalies dans des donnÃ©es temporelles et catÃ©goriques, avec stockage des donnÃ©es dans MongoDB et Elasticsearch.

Le projet se concentre sur l'intÃ©gration de donnÃ©es provenant de diverses sources (API, fichiers CSV, XML), leur stockage et leur fusion dans des bases de donnÃ©es.
**Une fonctionnalitÃ© clÃ©** du projet est le **suivi de l'empreinte carbone** des opÃ©rations de traitement, rÃ©alisÃ© Ã  l'aide de la bibliothÃ¨que **CodeCarbon**. Cela permet de mesurer l'impact environnemental des pipelines de donnÃ©es Ã  chaque exÃ©cution, contribuant ainsi Ã  un dÃ©veloppement plus durable.

## **Architecture du projet** ğŸ—ï¸ <a name="architecture-du-projet"></a>

Le projet suit une architecture modulaire basÃ©e sur Kedro, oÃ¹ chaque tÃ¢che de traitement de donnÃ©es est encapsulÃ©e dans des pipelines distincts pour favoriser la flexibilitÃ© et la maintenance.


### **Vue d'ensemble des pipelines :**

- **Pipeline ETL (`etl_pipeline`)** : Extraction, transformation et stockage des donnÃ©es dans MongoDB.
- **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** : Fusion et stockage des donnÃ©es dans Elasticsearch.

## **Installation et configuration** âš™ï¸ <a name="installation-et-configuration"></a>

### **PrÃ©requis :**

- **Python 3.8** ou version plus rÃ©cente
- **MongoDB** (cloud ou instance locale)
- **Elasticsearch** (cloud ou instance locale)
- **Docker** (optionnel pour containeriser le projet)

### **Ã‰tapes d'installation :**

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/keagnon/DetectionAnomalie.git
   cd DetectionAnomalie
   ```

2. **CrÃ©er un environnement virtuel :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Unix
   # Ou
   venv\Scripts\activate     # Windows
   ```

3. **Installer les dÃ©pendances :**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurer les variables d'environnement :**
   CrÃ©ez un fichier `.env` et renseignez les informations de connexion MongoDB et Elasticsearch :
   ```env
   MONGODB_USERNAME=nom_utilisateur_mongo
   MONGODB_PASSWORD=mot_de_passe_mongo
   MONGODB_CLUSTER=adresse_du_cluster_mongo
   MONGODB_DBNAME=nom_base_de_donnÃ©e

   ELASTIC_USERNAME=nom_utilisateur_elastic
   ELASTIC_PASSWORD=mot_de_passe_elastic
   ELASTIC_DEPLOYMENT_ENDPOINT=adresse_du_cluster_elastic
   ```

## **Structure du projet** ğŸ—‚ï¸ <a name="structure-du-projet"></a>

La structure du projet suit les conventions de Kedro :

```
data-collection-kedro/
â”‚
â”œâ”€â”€ conf/                                # Fichiers de configuration
â”‚   â”œâ”€â”€ base/
â”‚   â”‚   â”œâ”€â”€ catalog.yml                  # DÃ©finition des jeux de donnÃ©es et leurs emplacements
â”‚   â”‚   â”œâ”€â”€ parameters_data_fusion_pipeline.yml
â”‚   â”‚   â”œâ”€â”€ parameters_etl_pipeline.yml
â”‚   â”‚   â””â”€â”€ parameters.yml               # ParamÃ¨tres globaux du projet
â”‚   â””â”€â”€ local/                           # Configuration spÃ©cifique Ã  l'environnement local
â”‚
â”œâ”€â”€ data/                                # DonnÃ©es utilisÃ©es dans le projet
â”‚   â”œâ”€â”€ data_carburant_xml/
â”‚   â”œâ”€â”€ data_merge/
â”‚   â”œâ”€â”€ meteo/
â”‚   â”‚   â”œâ”€â”€ combined_meteo_data.csv
â”‚   â”‚   â”œâ”€â”€ courbe_de_charge_data_2018_2024.csv
â”‚   â”‚   â”œâ”€â”€ meteo_data_2018_2024.csv
â”‚   â”‚   â”œâ”€â”€ movement_data_2018_2024.csv
â”‚   â”‚   â””â”€â”€ prix_du_carburant_data_2018_2024.csv
â”‚   â””â”€â”€ README.md                       # Documentation sur les donnÃ©es
â”‚
â”œâ”€â”€ images/                             # Captures d'Ã©cran
â”‚   â”œâ”€â”€ data_fusion/
â”‚   â”œâ”€â”€ elastic_search/
â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â””â”€â”€ tests_unitaires/
â”‚
â”œâ”€â”€ logs/                               # Dossier de logs pour l'empreinte carbone et autres logs
â”‚   â”œâ”€â”€ carbon_logs/                    # Logs gÃ©nÃ©rÃ©s par CodeCarbon
â”‚   â”‚   â”œâ”€â”€ emissions.csv               # Fichier CSV contenant les Ã©missions de carbone
â”‚
â”œâ”€â”€ kedro-dataeng-env/                   # Environnement virtuel Kedro
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_collection_kedro/           # RÃ©pertoire principal des sources du projet
â”‚   â”‚   â”œâ”€â”€ pipelines/                   # Pipelines de traitement des donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ data_fusion_pipeline/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py             # Fonctions spÃ©cifiques au pipeline de fusion de donnÃ©es
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py          # DÃ©finition des pipelines
â”‚   â”‚   â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ nodes.py             # Fonctions spÃ©cifiques au pipeline ETL
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline.py          # DÃ©finition de la pipeline ETL
â”‚   â”‚   â””â”€â”€ pipeline_registry.py         # Enregistrement des pipelines Kedro
â”‚
â”œâ”€â”€ tests/                               # Tests unitaires pour le projet
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ data_fusion_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_pipeline.py         # Tests pour le pipeline de fusion de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ etl_pipeline/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_pipeline.py         # Tests pour le pipeline ETL
â”‚   â”‚   â”‚   â”œâ”€â”€ test_transform.py        # Tests pour les transformations de donnÃ©es ETL
â”‚
â”œâ”€â”€ Dockerfile                           # Fichier Docker pour containeriser le projet
â”œâ”€â”€ pyproject.toml                       # Fichier de configuration du projet et des dÃ©pendances
â”œâ”€â”€ README.md                            # Documentation principale du projet
â””â”€â”€ requirements.txt                     # Liste des dÃ©pendances Python du projet

```

## **ExÃ©cution du projet** ğŸš€ <a name="exÃ©cution-du-projet"></a>

### **ExÃ©cuter localement :**

- **ExÃ©cuter tous les pipelines :**
   ```bash
   kedro run
   ```

- **ExÃ©cuter un pipeline spÃ©cifique :**
   - **Pipeline ETL** :
     ```bash
     kedro run --pipeline=etl_pipeline
     ```

   - **Pipeline de Fusion de DonnÃ©es** :
     ```bash
     kedro run --pipeline=data_fusion_pipeline
     ```

---

### **ExÃ©cuter avec Docker :**

- **Construire l'image Docker :**
   ```bash
   docker build -t kedro-data-engineering .
   ```

- **ExÃ©cuter le conteneur Docker :**
   ```bash
   docker run -it kedro-data-engineering
   ```


## **Description des pipelines** ğŸ”„ <a name="description-des-pipelines"></a>

### **Pipeline ETL (`etl_pipeline`)** ğŸ› ï¸

- **Objectif** : Extraire des donnÃ©es API/CSV/XML, les transformer et les stocker dans MongoDB.
- **Fonctions principales** :
  - `fetch_data_from_api()`
  - `read_csv_file()`
  - `store_in_mongodb()`

### **Pipeline de Fusion de DonnÃ©es (`data_fusion_pipeline`)** ğŸ”—

- **Objectif** : Fusionner plusieurs jeux de donnÃ©es, normaliser les colonnes et les stocker dans Elasticsearch.
- **Fonctions principales** :
  - `load_collections()`
  - `select_columns()`
  - `normalize_columns()`
  - `merge_data_store_in_elastic()`


Les donnÃ©es brutes stockÃ©es dans Elasticsearch sont visualisÃ©es dans un tableau de bord **Kibana** hÃ©bergÃ© sur une machine virtuelle **GCP**. Voici une capture d'Ã©cran du dashboard Kibana :

![Capture du Dashboard Kibana](lien_capture_kibana)


## **Fichiers de configuration** ğŸ› ï¸ <a name="fichiers-de-configuration"></a>

### **1. `parameters_data_fusion_pipeline.yml`** :
- DÃ©finit les jeux de donnÃ©es, leurs sources et destinations (MongoDB, Elasticsearch).

### **2. `parameters_etl_pipeline.yml`** :
- Contient les paramÃ¨tres globaux comme la taille des chunks ou les URL des API.



## **Tests du projet** ğŸ§ª <a name="tests-du-projet"></a>

Les tests sont rÃ©alisÃ©s avec **pytest**. Les tests unitaires sont disponibles dans le rÃ©pertoire `tests/`.

### **ExÃ©cuter les tests :**

- **Tous les tests** :
   ```bash
   pytest tests/
   ```

- **Tester un pipeline spÃ©cifique** :
   ```bash
   pytest tests/pipelines/etl_pipeline/
   ```

## **Empreinte Carbone** ğŸŒ± <a name="empreinte-carbone"></a>

Ce projet utilise **CodeCarbon** pour suivre et enregistrer l'empreinte carbone des pipelines de traitement de donnÃ©es, afin de minimiser l'impact environnemental. Chaque exÃ©cution du pipeline suit la consommation d'Ã©nergie et calcule les Ã©missions de CO2 gÃ©nÃ©rÃ©es par l'infrastructure de traitement.

Les logs d'Ã©mission sont stockÃ©s dans le rÃ©pertoire **`logs/carbon_logs`** et peuvent Ãªtre visualisÃ©s pour chaque exÃ©cution.

### **ExÃ©cuter le pipeline avec suivi de l'empreinte carbone :**

Pour suivre l'empreinte carbone de l'exÃ©cution des pipelines, assurez-vous que **CodeCarbon** est bien installÃ© et configurez votre pipeline pour capturer ces informations :

```bash
kedro run  # Lance le pipeline tout en suivant l'empreinte carbone
```

Les rÃ©sultats seront enregistrÃ©s dans le fichier **`emissions.csv`** du rÃ©pertoire **`logs/carbon_logs/`**.

Voici un exemple d'information enregistrÃ©e dans les logs :

```
timestamp,duration (s),emissions (kg CO2e),energy_consumed (kWh)
2024-09-12 15:30:45,360,0.15,0.3
```

### Capture d'Ã©cran du terminal affichant l'empreinte carbone :

![Empreinte Carbone dans le Terminal](images/carbon_logs/im1.png)
![Empreinte Carbone dans le Terminal](images/carbon_logs/im2.png)
![Empreinte Carbone dans le Terminal](images/carbon_logs/im3.png)


## **Exemples d'images** ğŸ–¼ï¸ <a name="exemples-dimages"></a>

Ici vous trouverez les captures d'Ã©cran des exÃ©cutions de nos pipelines, ainsi que des rÃ©sultats de tests ou du coverage.


### **Exemple d'image - Tests unitaires et couverture :**

![Tests unitaires](images/tests_unitaires/im1.png)
![Coverage des tests](images/tests_unitaires/im2.png)


### **Exemple d'image - Visualisation MongoDB :**

![MongoDB](images/mongodb/im1.png)
![MongoDB](images/mongodb/im2.png)
![MongoDB](images/mongodb/im3.png)
![MongoDB](images/mongodb/im4.png)

### **Exemple d'image - ExÃ©cution du pipeline ETL :**

![Pipeline ETL](images/etl_pipeline/im1.png)
![Pipeline ETL](images/etl_pipeline/im2.png)
![Pipeline ETL](images/etl_pipeline/im3.png)
![Pipeline ETL](images/etl_pipeline/im4.png)
![Pipeline ETL](images/etl_pipeline/im5.png)
![Pipeline ETL](images/etl_pipeline/im6.png)


### **Exemple d'image - Visualisation Elasticsearch :**

![Elasticsearch](images/elastic_search/im1.png)
![Elasticsearch](images/elastic_search/im2.png)
![Elasticsearch](images/elastic_search/im3.png)
![Elasticsearch](images/elastic_search/im4.png)
![Elasticsearch](images/elastic_search/im5.png)
![Elasticsearch](images/elastic_search/im6.png)
![Elasticsearch](images/elastic_search/im7.png)

### **Exemple d'image - ExÃ©cution du pipeline de fusion :**

![Pipeline de fusion](images/data_fusion/im1.png)
![Pipeline de fusion](images/data_fusion/im2.png)
![Pipeline de fusion](images/data_fusion/im3.png)
![Pipeline de fusion](images/data_fusion/im4.png)
![Pipeline de fusion](images/data_fusion/im5.png)
![Pipeline de fusion](images/data_fusion/im6.png)
![Pipeline de fusion](images/data_fusion/im7.png)
![Pipeline de fusion](images/data_fusion/im8.png)
![Pipeline de fusion](images/data_fusion/im12.png)
![Pipeline de fusion](images/data_fusion/im11.png)



